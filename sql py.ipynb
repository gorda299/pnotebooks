{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to database\n",
    "\n",
    "Creating connection to Microsoft SQL Server and two databases with positive and negative sets. We should use generators for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(constring, query):\n",
    "    cnxn = pyodbc.connect(constring)\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    y, t = [], []\n",
    "    for row in cursor:\n",
    "        r_text = row.ttext\n",
    "        r_type = row.ttype\n",
    "        y.append(r_text)\n",
    "        t.append(r_type)\n",
    "        \n",
    "        if len(y) == batch_size:\n",
    "            npx = np.array(y)\n",
    "            npy = np.array(t)\n",
    "            yield npx, npy\n",
    "            y, t = [], []\n",
    "    pyodbc.Connection.close(cnxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "We should encode words as their indexes (computed by overall frequency in the dataset).\n",
    "Using russian http://www.ruscorpora.ru/en/\n",
    "#### Step 1. Clear dataset. \n",
    "#### Step 2. Select meaningful words.\n",
    "#### Step 3. Calculate frequency of each word\n",
    "#### Step 4. Replace words by indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "hash_map = {}\n",
    "max_features = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Clear dataset. \n",
    "Select words one by one. Symbols are meaningful because of smiles and emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(file_text):\n",
    "    if file_text is not None:\n",
    "        #firstly let's apply nltk tokenization\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "        #let's delete punctuation symbols\n",
    "        stop_words = ([',','\\\\','/','*','','-','http',';',':','@'])\n",
    "        tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "        #deleting stop_words\n",
    "        #stop_words = stopwords.words('russian')\n",
    "        #stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'в', '—', 'к', 'на', 'http'])\n",
    "       \n",
    "       # tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "        #cleaning words\n",
    "        #tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "\n",
    "        return tokens\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a HashMap by using a Python dictionary to store the word frequencies of a book.\n",
    "A dictionary is an associative array (also known as hashes).\n",
    "Any key of the dictionary is associated, or mapped, to a value.\n",
    "The values of a dictionary can be any Python data type, so dictionaries are unordered key-value-pairs.\n",
    "\n",
    "By creating the dictionary, we will store the words as the keys and the value will represent the count. By doing this, we can retrieve any word without having to recount every single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def map_words(tokens):\n",
    "    \n",
    "    if tokens is not None:\n",
    "        for element in tokens:\n",
    "            # Remove Punctuation\n",
    "            word = element.replace(\",\",\"\")\n",
    "            word = word.replace(\".\",\"\")\n",
    "            word = word.lower()\n",
    "            # Word Exist?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnstr_positive = 'Trusted_Connection=yes;DRIVER={SQL Server};SERVER=DESKTOP-1RHDOBR\\GORDASQL;DATABASE=positive;UID=pyuser;PWD=pypypy'\n",
    "\n",
    "cnstr_negative = 'Trusted_Connection=yes;DRIVER={SQL Server};SERVER=DESKTOP-1RHDOBR\\GORDASQL;DATABASE=negative;UID=pyuser;PWD=pypypy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_frequency():\n",
    "    hash_map.clear()\n",
    "    \n",
    "    query_positive = \"SELECT [ttext], [ttype] FROM [dbo].[sortpos]\"\n",
    "    pdg = data_generator(cnstr_positive, query_positive)\n",
    "    \n",
    "    query_negative = \"SELECT [ttext], [ttype] FROM [dbo].[sortneg]\"\n",
    "    ndg = data_generator(cnstr_negative, query_negative)\n",
    "    \n",
    "    for current_positive_set in pdg:\n",
    "        for sentence in current_positive_set[0]:\n",
    "            words = tokenize(sentence)\n",
    "            map = map_words(words)\n",
    "            \n",
    "    for current_negative_set in ndg:\n",
    "        for sentence in current_negative_set[0]:\n",
    "            words = tokenize(sentence)\n",
    "            map = map_words(words)\n",
    "    \n",
    "    min_frequency = 2#max(map.values()) - max_features if max(map.values()) > max_features else 2\n",
    "    map = {key: value for key, value in map.items() if value > min_frequency}\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = fill_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [привет] Frequency: 974\n",
      "Word: [,] Frequency: 0\n",
      "Word: [дела] Frequency: 571\n",
      "Word: [)] Frequency: 151358\n",
      "Word: [(] Frequency: 181951\n"
     ]
    }
   ],
   "source": [
    "word_list = ['привет',',','дела',')','(']\n",
    "\n",
    "for word in word_list:\n",
    "    print('Word: [' + word + '] Frequency: ' + str(map.get(word,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "maxlen = 60  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    max_frequency = max(map.values())\n",
    "    \n",
    "    train_cnt = 500\n",
    "    #cnxn = pyodbc.connect(cnstr_positive)\n",
    "    #query = \"SELECT count(*) as cnt FROM [dbo].[mixedmessages]\"\n",
    "    #cursor = cnxn.cursor()\n",
    "    #cursor.execute(query)\n",
    "    #for row in cursor:\n",
    "    #    train_cnt = row.cnt/2\n",
    "    #pyodbc.Connection.close(cnxn)\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = ([] for i in range(4))\n",
    "    \n",
    "    query = \"SELECT top 1000 [ttext], [ttype] FROM [dbo].[mixedmessages] order by newid()\"\n",
    "    mdg = data_generator(cnstr_positive, query)\n",
    "    \n",
    "    for current_set in mdg:\n",
    "        for sentence in current_set[0]:\n",
    "            words = tokenize(sentence)\n",
    "            w = []\n",
    "            #print(words)\n",
    "            for word in words:\n",
    "                #print(word, map.get(word,0))\n",
    "                w.append(max_frequency-map.get(word,0))\n",
    "            if len(x_train)<train_cnt:\n",
    "                x_train.append(w)\n",
    "            else:\n",
    "                x_test.append(w)\n",
    "                \n",
    "        for value in current_set[1]:\n",
    "            if len(y_train)<train_cnt:\n",
    "                y_train.append(value)\n",
    "            else:\n",
    "                y_test.append(value)\n",
    "                \n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    #print(x_train)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = train_model()\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 500 samples, validate on 492 samples\n",
      "Epoch 1/15\n",
      "500/500 [==============================] - 6s - loss: 0.6552 - acc: 0.7260 - val_loss: 0.6233 - val_acc: 0.6606\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - 3s - loss: 0.5646 - acc: 0.8400 - val_loss: 0.5445 - val_acc: 0.7256\n",
      "Epoch 3/15\n",
      "500/500 [==============================] - 3s - loss: 0.4479 - acc: 0.8540 - val_loss: 0.3942 - val_acc: 0.8841\n",
      "Epoch 4/15\n",
      "500/500 [==============================] - 3s - loss: 0.3348 - acc: 0.8960 - val_loss: 0.2940 - val_acc: 0.9146\n",
      "Epoch 5/15\n",
      "500/500 [==============================] - 3s - loss: 0.2423 - acc: 0.9200 - val_loss: 0.2359 - val_acc: 0.9207\n",
      "Epoch 6/15\n",
      "500/500 [==============================] - 3s - loss: 0.1953 - acc: 0.9280 - val_loss: 0.2082 - val_acc: 0.9289\n",
      "Epoch 7/15\n",
      "500/500 [==============================] - 3s - loss: 0.1949 - acc: 0.9380 - val_loss: 0.2157 - val_acc: 0.9431\n",
      "Epoch 8/15\n",
      "500/500 [==============================] - 3s - loss: 0.1722 - acc: 0.9440 - val_loss: 0.1935 - val_acc: 0.9390\n",
      "Epoch 9/15\n",
      "500/500 [==============================] - 3s - loss: 0.1839 - acc: 0.9460 - val_loss: 0.1818 - val_acc: 0.9390\n",
      "Epoch 10/15\n",
      "500/500 [==============================] - 3s - loss: 0.1714 - acc: 0.9540 - val_loss: 0.1676 - val_acc: 0.9553\n",
      "Epoch 11/15\n",
      "500/500 [==============================] - 4s - loss: 0.1556 - acc: 0.9580 - val_loss: 0.1653 - val_acc: 0.9472\n",
      "Epoch 12/15\n",
      "500/500 [==============================] - 3s - loss: 0.1447 - acc: 0.9560 - val_loss: 0.1474 - val_acc: 0.9614\n",
      "Epoch 13/15\n",
      "500/500 [==============================] - 3s - loss: 0.1449 - acc: 0.9660 - val_loss: 0.1605 - val_acc: 0.9512\n",
      "Epoch 14/15\n",
      "500/500 [==============================] - 3s - loss: 0.1417 - acc: 0.9600 - val_loss: 0.1441 - val_acc: 0.9573\n",
      "Epoch 15/15\n",
      "500/500 [==============================] - 3s - loss: 0.1328 - acc: 0.9620 - val_loss: 0.1408 - val_acc: 0.9573\n",
      "480/492 [============================>.] - ETA: 0sTest score: 0.140795229169\n",
      "Test accuracy: 0.95731707414\n"
     ]
    }
   ],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 0s     \n",
      "Test score: 0.140795229169\n",
      "Test accuracy: 0.95731707414\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_example = np.array([[23, 75, 43, 225, 322]])\n",
    "my_example = sequence.pad_sequences(my_example, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(my_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53944892]], dtype=float32)"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(my_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pymorphy2\n",
    "#morph = pymorphy2.MorphAnalyzer()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
